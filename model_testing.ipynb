{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweet_processor import generateTweetTensor\n",
    "from preprocess3 import genLabels\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import models\n",
    "\n",
    "from torchtext import data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculateAcc(a, p):\n",
    "\n",
    "    b = len(a)\n",
    "    correct = 0\n",
    "    for i in range(0, b):\n",
    "        if a[i] == p[i]:\n",
    "            print(correct)\n",
    "            correct += 1\n",
    "        \n",
    "    print(b)\n",
    "    acc = correct / b\n",
    "    return acc\n",
    "\n",
    "#Load Pre-trained model for Words\n",
    "print(\"Loading Pre-trained model of many tweets...\")\n",
    "glove = KeyedVectors.load_word2vec_format('glove.twitter.27B.100d.w2vformat.txt')\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Load tweets from json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAcc(a, p):\n",
    "\n",
    "    b = len(a)\n",
    "    correct = 0\n",
    "    for i in range(0, b):\n",
    "        if a[i] == p[i]:         \n",
    "            correct += 1\n",
    "        \n",
    "    #print(\"Length\")\n",
    "    #print(b)\n",
    "    #print(\"Correct:\")\n",
    "    #print(correct)\n",
    "    acc = correct / b\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_classic2(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(RNN_classic2, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim)\n",
    "        self.hidden = torch.zeros(1, 1, hidden_dim)\n",
    "        self.fc0 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 3)\n",
    "\n",
    "    \n",
    "    def forward(self, data, lengths=None):\n",
    "        length = lengths.cpu()\n",
    "        data = torch.transpose(data, 0, 1)\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(data, lengths= length, enforce_sorted = False) # unpad\n",
    "        x_float = x.float()\n",
    "        a, x_next = self.gru(x_float)\n",
    "        final = self.fc0(x_next)\n",
    "        final = self.fc1(final)\n",
    "        final = F.softmax(final, dim = -1)\n",
    "        final = final[0]\n",
    "        return final\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.ones(1, self.hidden_size, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_classic2(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(RNN_classic2, self).__init__()\n",
    "        self.fc0 = nn.Linear(100 * 58, 100)\n",
    "        self.fc1 = nn.Linear(100, 3)\n",
    "\n",
    "    \n",
    "    def forward(self, data, lengths=None):\n",
    "        #length = lengths.cpu()\n",
    "        \n",
    "        #data = torch.transpose(data, 0, 1)\n",
    "        print(data.shape)\n",
    "        data = data.view(58*100, -1)\n",
    "        print(data.shape)\n",
    "        final = self.fc0(data.float() )\n",
    "        final = self.fc1(final)\n",
    "        final = F.softmax(final, dim = -1)\n",
    "        final = final[0]\n",
    "        return final\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_labels):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "\n",
    "        self.num_layers = 1\n",
    "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=self.num_layers)\n",
    "        self.linear = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "\n",
    "        x = x.float()\n",
    "        output, h_n = self.gru(x)\n",
    "        x = h_n[0].float()\n",
    "        #print(x.shape)\n",
    "        x = self.linear(x)\n",
    "        #print(x.shape)\n",
    "        x = F.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Netc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Netc, self).__init__()\n",
    "        #self.conv1 = nn.Conv2d(58, 64, 100, 1)  # 3 channels in input image RGB, 10 kernels, by default input is on, kernel is size 3x3 (Default stride = 1, pooling = 0)\n",
    "        #self.conv2 = nn.Conv2d(10, 10, 5, 1)  # 10 channels, since there is 10 kernels on the first layer\n",
    "        self.fc1 = nn.Linear(100*58, 100)\n",
    "        self.fc2 = nn.Linear(100, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "\n",
    "    def forward(self, values, lengths = None):  # where x is input image batch\n",
    "        #x = values.float()\n",
    "        #x = F.relu(self.conv1(x))\n",
    "        #x = self.pool(x)\n",
    "        #x = F.relu(self.conv2(x))\n",
    "        #x = self.pool(x)\n",
    "        x = values.view(-1, 100 * 58)\n",
    "        x = F.relu(self.fc1(x.float()))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr= 0.1)\n",
    "#optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "#tweets = pd.read_csv('overtrain.csv')\n",
    "\n",
    "tweets = pd.read_json('trump_tweets_json.json')\n",
    "tweets = tweets[['created_at', 'text']]\n",
    "tweets = tweets.iloc[:80, :]\n",
    "#tweets_with_labels = tweets[['onehot','text']]\n",
    "tweets_with_labels = genLabels(tweets)\n",
    "#print(\"Loaded dataset.\")\n",
    "tweets_with_labels\n",
    "#tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split sets into train, test, and validation\n",
    "print(\"Splitting data...\")\n",
    "rest_x, test_x, rest_y, test_y = train_test_split(tweets_with_labels, tweets_with_labels['onehot'], test_size=0.2, random_state=37)\n",
    "train_x, validate_x, train_y, validate_y = train_test_split(rest_x, rest_y, test_size=0.2, random_state=37)\n",
    "\n",
    "\n",
    "print(\"Generating vectors...\")\n",
    "# Generate vector of tweets\n",
    "loss_fnc = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating Test Vector...\")\n",
    "test_tweet_vector, test_lengths, test_nulls = generateTweetTensor(glove, test_x)\n",
    "test_y_tensor = torch.LongTensor([x for x in np.array(test_y)])\n",
    "test_y_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0, len(test_nulls)):\n",
    "    i  = test_nulls[n] - n\n",
    "    test_y_tensor = torch.cat([test_y_tensor[0: i], test_y_tensor[i+1:]])\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_tweet_vector, test_y_tensor, torch.tensor(test_lengths))\n",
    "test_iter = DataLoader(test_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Generating Validate Vector...\")\n",
    "validate_tweet_vector, validate_lengths, validate_nulls = generateTweetTensor(glove, validate_x)\n",
    "validate_y_tensor = torch.LongTensor([x for x in np.array(validate_y)])\n",
    "for n in range(0, len(validate_nulls)):\n",
    "    i  = validate_nulls[n] - n\n",
    "    validate_y_tensor = torch.cat([validate_y_tensor[0: i], validate_y_tensor[i+1:]])\n",
    "val_dataset = torch.utils.data.TensorDataset(validate_tweet_vector, validate_y_tensor, torch.tensor(validate_lengths))\n",
    "#val_iter = data.BucketIterator(val_dataset, batch_size=64, repeat=False, sort_key=lambda x: len(x.text))\n",
    "val_iter = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "print(\"Generating Train Vector...\")\n",
    "train_tweet_vector, train_lengths, train_nulls = generateTweetTensor(glove, train_x)\n",
    "train_y_tensor = torch.LongTensor([x for x in np.array(train_y)])\n",
    "#actual = (torch.from_numpy(np.eye(10)[label])).float()\n",
    "\n",
    "\n",
    "for n in range(0, len(train_nulls)):\n",
    "    i  = train_nulls[n] - n\n",
    "    train_y_tensor = torch.cat([train_y_tensor[0: i], train_y_tensor[i+1:]])\n",
    "train_dataset = torch.utils.data.TensorDataset(train_tweet_vector, train_y_tensor, torch.tensor(train_lengths) )\n",
    "train_iter = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "print(\"Generated buckets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RNNClassifier(100, 400, 3)\n",
    "\n",
    "# Create rnn\n",
    "print(model.parameters())\n",
    "learning_rate = 0.2\n",
    "num_epochs = 2000\n",
    "\n",
    "#model = RNNClassifier(100, 100, 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "# Train models\n",
    "loss_fnc = nn.CrossEntropyLoss()\n",
    "loss_fnc = nn.BCELoss()\n",
    "#hidden = model.init_hidden()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "eps = []\n",
    "training_loss = []\n",
    "training_accs = []\n",
    "validation_loss = []\n",
    "validation_accs = []\n",
    "print(\"Model Created.\")\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    eps += [e]\n",
    "    labels = []\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Epoch: \" + str(e))\n",
    "    for j, batch in enumerate(train_iter):\n",
    "        #print(\"batch:\")\n",
    "        #optimizer.zero_grad()\n",
    "        inputs_float = batch[0]\n",
    "        #print(inputs_float.size())\n",
    "\n",
    "        actual = batch[1]\n",
    "        lengths_float = batch[2]\n",
    "        lengths = lengths_float.long()\n",
    "        inputs = inputs_float.long()\n",
    "        optimizer.zero_grad()\n",
    "        predicted = model(inputs)\n",
    "        labels = []\n",
    "        for l in actual.tolist():\n",
    "            labels += [l.index(max(l))]\n",
    "        label_tensor = torch.Tensor(labels).long()\n",
    "        loss = loss_fnc(input = predicted.float(), target = actual.float())\n",
    "        #loss = loss_fnc(predicted.float(), label_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    print(\"Calculating Training Accuracy...\")\n",
    "    # Calculate Training Accuracy\n",
    "    train_labels = []\n",
    "    train_preds = []\n",
    "\n",
    "\n",
    "    for k, t_batch in enumerate(train_iter):\n",
    "        t_inputs = t_batch[0]\n",
    "        t_actual = t_batch[1]\n",
    "        t_lengths_floats = t_batch[2]\n",
    "        t_lengths = t_lengths_floats.long()\n",
    "\n",
    "        train_labels += t_actual.tolist()\n",
    "        t_predicted = model(t_inputs)\n",
    "        train_preds += t_predicted.tolist()\n",
    "\n",
    "    \n",
    "    t_l_best = []\n",
    "    t_p_best = []\n",
    "    for l in train_labels:\n",
    "        t_l_best += [l.index(max(l))]\n",
    "    for p in train_preds:\n",
    "        t_p_best += [p.index(max(p))]  \n",
    "    print(len(train_preds))\n",
    "    print(len(train_lengths))\n",
    "    t_acc = calculateAcc(t_l_best, t_p_best)\n",
    "    t_loss = loss_fnc(torch.FloatTensor(train_preds), torch.LongTensor(t_l_best))\n",
    "\n",
    "    training_accs += [t_acc]\n",
    "    training_loss += [t_loss.item()]\n",
    "    print(\"Training Accuracy: \" + str(t_acc))\n",
    "    print(\"Training Loss: \" + str(t_loss.item()))\n",
    "\n",
    "    # Calculate Validation Accuracy\n",
    "    valid_labels = []\n",
    "    valid_preds = []\n",
    "    for k, v_batch in enumerate(val_iter):\n",
    "        v_inputs = v_batch[0]\n",
    "        v_actual = v_batch[1]\n",
    "        v_lengths_floats = v_batch[2]\n",
    "        v_lengths = v_lengths_floats.long()\n",
    "        valid_labels += v_actual.tolist()\n",
    "        v_predicted = model(v_inputs)\n",
    "        valid_preds += v_predicted.tolist()\n",
    "\n",
    "    v_l_best = []\n",
    "    v_p_best = []\n",
    "    for v in valid_labels:\n",
    "        v_l_best += [v.index(max(v))]\n",
    "    for p in valid_preds:\n",
    "        v_p_best += [p.index(max(p))]  \n",
    "    print(\"Valid Accuracy:\")\n",
    "    v_acc = calculateAcc(v_l_best, v_p_best)\n",
    "    #convert ohe labels to normal\n",
    "    labels = []\n",
    "    for l in valid_labels:\n",
    "        labels += [l.index(max(l))]\n",
    "    v_label_tensor = torch.Tensor(labels).long()\n",
    "    \n",
    "    v_loss = loss_fnc(torch.FloatTensor(valid_preds), torch.LongTensor(v_label_tensor))\n",
    "    validation_accs += [v_acc]\n",
    "    validation_loss += [v_loss.item()]\n",
    "    print(\"Validation Accuracy: \" + str(v_acc))\n",
    "    print(\"Validation Loss: \" + str(v_loss.item()))\n",
    "\n",
    "# Calculate Testing Accuracy\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "for k, t_batch in enumerate(test_iter):\n",
    "    t_inputs = t_batch[0]\n",
    "    t_actual = t_batch[1]\n",
    "    t_lengths_floats = t_batch[2]\n",
    "    t_lengths = t_lengths_floats.long()\n",
    "    test_labels += t_actual.tolist()\n",
    "    t_predicted = model(t_inputs)\n",
    "    test_preds += t_predicted.tolist()\n",
    "\n",
    "labels = []\n",
    "for l in test_labels:\n",
    "    labels += [l.index(max(l))]\n",
    "test_label_tensor = torch.Tensor(labels).long()\n",
    "    \n",
    "test_loss = loss_fnc(torch.FloatTensor(test_preds), torch.LongTensor(test_label_tensor))\n",
    "print(\"TESTING LOSS: \" + str(test_loss.item()))\n",
    "t_acc = calculateAcc(test_labels, test_preds)\n",
    "print(\"TESTING ACCURACY: \" + str(t_acc))\n",
    "\n",
    "#   Display Accuracy vs. Epoch\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(eps, training_accs, label='Training Data')\n",
    "ax.plot(eps, validation_accs, label='Validation Data')\n",
    "ax.set(xlabel='Number of Epochs', ylabel='Accuracy', title='Accuracy vs. Epoch')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "#   Display Loss vs. Epoch\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(eps, training_loss, label='Training Data')\n",
    "ax.plot(eps, validation_loss, label='Validation Data')\n",
    "ax.set(xlabel='Number of Epochs', ylabel='Loss', title='Loss vs. Epoch')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_classic(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(RNN_classic, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim)\n",
    "        self.hidden = torch.zeros(1, 1, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 3)\n",
    "\n",
    "\n",
    "    def forward(self, data, lengths=None):\n",
    "        length = lengths.cpu()\n",
    "        data = torch.transpose(data, 0, 1)\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(data, lengths= length, enforce_sorted = False) # unpad\n",
    "        x_float = x.float()\n",
    "        a, x_next = self.gru(x_float)\n",
    "        final = self.fc1(x_next)\n",
    "        final = F.softmax(final, dim = 1)\n",
    "        final = final[0]\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN2, self).__init__()\n",
    "        print(\"Intializing..\")\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def init_hidden(self):\n",
    "            return torch.ones(1, self.hidden_size, dtype=torch.long)\n",
    "\n",
    "    def forward(self, data, length, hidden):\n",
    "        length = length.cpu()\n",
    "   \n",
    "\n",
    "        # data = torch.transpose(data, 0, 1)\n",
    "        # x = torch.nn.utils.rnn.pack_padded_sequence(data, lengths= length, enforce_sorted = False) # unpad\n",
    "        combined = torch.cat((data.view(1, -1), hidden), 1)\n",
    "        combined = combined.float()\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_classic2(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(RNN_classic2, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim)\n",
    "        self.hidden = torch.zeros(1, 1, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 3)\n",
    "\n",
    "    \n",
    "    def forward(self, data, lengths=None):\n",
    "        length = lengths.cpu()\n",
    "        data = torch.transpose(data, 0, 1)\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(data, lengths= length, enforce_sorted = False) # unpad\n",
    "        x_float = x.float()\n",
    "        a, x_next = self.gru(x_float)\n",
    "        final = self.fc1(x_next)\n",
    "        final = F.softmax(final, dim = 1)\n",
    "        final = final[0]\n",
    "        return final\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.ones(1, self.hidden_size, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
