{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tweet_processor import generateTweetTensor\n",
    "from preprocess import genLabels\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-trained model of many tweets...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculateAcc(a, p):\n",
    "\n",
    "    b = len(a)\n",
    "    correct = 0\n",
    "    for i in range(0, b):\n",
    "        if a[i] < 0.5  and p[i] < 0.5:\n",
    "            correct += 1\n",
    "        elif a[i] > 0.5  and p[i] > 0.5:\n",
    "            correct += 1\n",
    "\n",
    "    acc = correct / b\n",
    "    return acc\n",
    "\n",
    "#Load Pre-trained model for Words\n",
    "print(\"Loading Pre-trained model of many tweets...\")\n",
    "glove = KeyedVectors.load_word2vec_format('glove.twitter.27B.100d.w2vformat.txt')\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Load tweets from json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x1a30bf59a8>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = RNN2(100, 100, 3)\n",
    "print(model.parameters())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.01)\n",
    "optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded dataset.\n",
      "Splitting data...\n",
      "Generating vectors...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2473    [[]]\n",
       "5707      []\n",
       "5671    [[]]\n",
       "7043      []\n",
       "9320      []\n",
       "1155      []\n",
       "1301    [[]]\n",
       "1570    [[]]\n",
       "7307    [[]]\n",
       "7240      []\n",
       "830       []\n",
       "2546      []\n",
       "7216    [[]]\n",
       "7395      []\n",
       "2745      []\n",
       "6970    [[]]\n",
       "5019    [[]]\n",
       "349       []\n",
       "985       []\n",
       "4300      []\n",
       "4813    [[]]\n",
       "3416    [[]]\n",
       "8557    [[]]\n",
       "6785      []\n",
       "3083      []\n",
       "1706    [[]]\n",
       "7329    [[]]\n",
       "5556      []\n",
       "4907    [[]]\n",
       "3738      []\n",
       "        ... \n",
       "3798    [[]]\n",
       "8988      []\n",
       "7828    [[]]\n",
       "3269      []\n",
       "3224      []\n",
       "432       []\n",
       "8321      []\n",
       "453       []\n",
       "8894      []\n",
       "5814    [[]]\n",
       "7184    [[]]\n",
       "48        []\n",
       "3194      []\n",
       "7137    [[]]\n",
       "6244      []\n",
       "290     [[]]\n",
       "2366      []\n",
       "8802      []\n",
       "799     [[]]\n",
       "2589    [[]]\n",
       "6096      []\n",
       "1237      []\n",
       "4380      []\n",
       "8848      []\n",
       "3394      []\n",
       "2659    [[]]\n",
       "3221      []\n",
       "6589      []\n",
       "7074    [[]]\n",
       "4447      []\n",
       "Name: onehot, Length: 5987, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"Loading dataset...\")\n",
    "tweets = pd.read_json('trump_tweets_json.json')\n",
    "tweets = tweets[['created_at', 'text']]\n",
    "tweets_with_labels = genLabels(tweets)\n",
    "print(\"Loaded dataset.\")\n",
    "\n",
    "\n",
    "# Split sets into train, test, and validation\n",
    "print(\"Splitting data...\")\n",
    "rest_x, test_x, rest_y, test_y = train_test_split(tweets_with_labels, tweets_with_labels['onehot'], test_size=0.2, random_state=37)\n",
    "train_x, validate_x, train_y, validate_y = train_test_split(rest_x, rest_y, test_size=0.2, random_state=37)\n",
    "\n",
    "\n",
    "print(\"Generating vectors...\")\n",
    "# Generate vector of tweets\n",
    "loss_fnc = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_x[\"onehot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Test Vector...\n",
      "              created_at                                               text  \\\n",
      "2232 2019-06-26 23:46:05  ....They came to me asking for help, and I got...   \n",
      "3067 2019-04-22 18:05:04  Spoke to Prime Minister Ranil Wickremesinghe o...   \n",
      "3593 2019-02-26 01:50:37  ....This will be remembered as one of the most...   \n",
      "2776 2019-05-14 20:45:21  It is great to be here in Hackberry, Louisiana...   \n",
      "3174 2019-04-12 16:38:02  Due to the fact that Democrats are unwilling t...   \n",
      "6567 2018-04-13 12:01:47  James Comey is a proven LEAKER &amp; LIAR. Vir...   \n",
      "7503 2017-11-08 03:09:37  The U.S., under my administration, is complete...   \n",
      "7559 2017-11-02 11:54:36  ...There is also something appropriate about k...   \n",
      "882  2019-09-25 03:17:40  Sooooo true @LindseyGrahamSC! https://t.co/ZzF...   \n",
      "934  2019-09-21 21:38:54  �They�re trying to turn what was a Biden scand...   \n",
      "9141 2017-02-17 21:48:22  The FAKE NEWS media (failing @nytimes, @NBCNew...   \n",
      "1177 2019-09-05 23:49:22  Great job done by @GovRonDesantis, @SenRickSco...   \n",
      "5194 2018-09-04 20:44:49  �Pledge to America�s Workers� https://t.co/wbG...   \n",
      "6581 2018-04-11 19:47:07  �Trump just took a giant step towards actual w...   \n",
      "1163 2019-09-06 14:29:46  The Fake News Media was fixated on the fact th...   \n",
      "6830 2018-02-27 22:58:19  .@SenatorWicker of Mississippi has been a grea...   \n",
      "6129 2018-06-06 13:48:20  The Fake News Media has been so unfair, and vi...   \n",
      "4983 2018-09-24 20:44:24  US-Korea Free Trade Agreement Signing Ceremony...   \n",
      "2052 2019-07-11 12:08:46  Democrats had to quickly take down a tweet cal...   \n",
      "8877 2017-04-23 19:48:16  New polls out today are very good considering ...   \n",
      "1470 2019-08-20 13:14:02  Congratulations, Great Show! https://t.co/RrdF...   \n",
      "9313 2017-01-22 12:35:09  Had a great meeting at CIA Headquarters yester...   \n",
      "6043 2018-06-14 15:09:12  The sleazy New York Democrats, and their now d...   \n",
      "3275 2019-04-02 12:54:11  There is no amount of testimony or document pr...   \n",
      "3074 2019-04-22 12:52:46  So true - thanks @SteveHiltonx @NextRevFNC! ht...   \n",
      "1332 2019-08-27 23:36:43  ....Another is a one-time BAD Congressman from...   \n",
      "3046 2019-04-23 11:47:36                                KEEP AMERICA GREAT!   \n",
      "7325 2017-12-03 13:00:47  After years of Comey, with the phony and disho...   \n",
      "6807 2018-03-02 11:18:28  Eric, we are all with you and your family! Loo...   \n",
      "3778 2019-01-31 19:19:19                            https://t.co/6wK5He4pk5   \n",
      "...                  ...                                                ...   \n",
      "3290 2019-04-01 13:03:28  Can you believe that the Radical Left Democrat...   \n",
      "2160 2019-07-03 14:12:12  S&amp;P 500 hits new record high. Up 19% for t...   \n",
      "1037 2019-09-14 14:57:35  I had a call today with Prime Minister Netanya...   \n",
      "6465 2018-04-25 19:33:07  Thank you Kanye, very cool! https://t.co/vRIC8...   \n",
      "7231 2017-12-20 17:30:26  Together, we are MAKING AMERICA GREAT AGAIN! h...   \n",
      "4464 2018-11-14 23:26:38  Our pledge to hire American includes those lea...   \n",
      "6982 2018-02-02 03:32:48  The Democrats just aren�t calling about DACA. ...   \n",
      "219  2019-10-27 21:24:55  Thank you to @MarthaRaddatz and @TerryMoran fo...   \n",
      "3492 2019-03-09 22:19:10  The Witch Hunt continues! https://t.co/9W1iUgE0d6   \n",
      "1013 2019-09-16 11:03:18  ....work that way. I have a better idea. Look ...   \n",
      "4104 2018-12-27 19:35:46  The reason the DACA for Wall deal didn�t get d...   \n",
      "4229 2018-12-14 22:18:15  I am pleased to announce that Mick Mulvaney, D...   \n",
      "454  2019-10-14 10:54:24  Former Democrat Senator Harry Reid just stated...   \n",
      "2705 2019-05-21 15:25:09  To the great people of Kentucky, please go out...   \n",
      "6359 2018-05-11 23:49:40  Why doesn�t the Fake News Media state that the...   \n",
      "4645 2018-10-30 01:56:19  .@troybalderson is doing a great job as Congre...   \n",
      "2148 2019-07-04 11:20:58  So important for our Country that the very sim...   \n",
      "9237 2017-02-04 12:59:35  When a country is no longer able to say who ca...   \n",
      "8152 2017-08-22 19:15:40  We pray for our fallen heroes who died while s...   \n",
      "5350 2018-08-22 00:45:19  Thank you West Virginia. I love you! https://t...   \n",
      "5576 2018-08-01 15:23:13  �We already have a smocking gun about a campai...   \n",
      "7925 2017-09-23 22:20:47  Democrats are laughingly saying that McCain ha...   \n",
      "7133 2018-01-06 04:19:10            Good idea Rand! https://t.co/55sqUDiC0s   \n",
      "3679 2019-02-17 12:00:26  Important meetings and calls on China Trade De...   \n",
      "3553 2019-03-04 00:08:18  ....President. We are WINNING big, the envy of...   \n",
      "7622 2017-10-27 14:33:15     Thank you @FLGovScott. https://t.co/rgdU9plvFK   \n",
      "9041 2017-03-15 23:25:25  Thank you Andrew Jackson! #POTUS7 #USA???? htt...   \n",
      "1079 2019-09-11 05:08:48  Greg Murphy won big, 62% to 37%, in North Caro...   \n",
      "7919 2017-09-24 18:24:36  Please to inform that the Champion Pittsburgh ...   \n",
      "2891 2019-05-08 12:57:35  �The real �Obstruction of Justice� is what the...   \n",
      "\n",
      "                datetime  label onehot  \n",
      "2232 2019-06-26 18:46:05     -1   [[]]  \n",
      "3067 2019-04-22 13:05:04      0     []  \n",
      "3593 2019-02-25 20:50:37      0     []  \n",
      "2776 2019-05-14 15:45:21      1     []  \n",
      "3174 2019-04-12 11:38:02      1     []  \n",
      "6567 2018-04-13 07:01:47     -1   [[]]  \n",
      "7503 2017-11-07 22:09:37      0     []  \n",
      "7559 2017-11-02 06:54:36      0     []  \n",
      "882  2019-09-24 22:17:40      1     []  \n",
      "934  2019-09-21 16:38:54     -1   [[]]  \n",
      "9141 2017-02-17 16:48:22     -1   [[]]  \n",
      "1177 2019-09-05 18:49:22      0     []  \n",
      "5194 2018-09-04 15:44:49      0     []  \n",
      "6581 2018-04-11 14:47:07      1     []  \n",
      "1163 2019-09-06 09:29:46      0     []  \n",
      "6830 2018-02-27 17:58:19     -1   [[]]  \n",
      "6129 2018-06-06 08:48:20      0     []  \n",
      "4983 2018-09-24 15:44:24      0     []  \n",
      "2052 2019-07-11 07:08:46     -1   [[]]  \n",
      "8877 2017-04-23 14:48:16     -1   [[]]  \n",
      "1470 2019-08-20 08:14:02      1     []  \n",
      "9313 2017-01-22 07:35:09      0     []  \n",
      "6043 2018-06-14 10:09:12     -1   [[]]  \n",
      "3275 2019-04-02 07:54:11      0     []  \n",
      "3074 2019-04-22 07:52:46      0     []  \n",
      "1332 2019-08-27 18:36:43      1     []  \n",
      "3046 2019-04-23 06:47:36     -1   [[]]  \n",
      "7325 2017-12-03 08:00:47     -1   [[]]  \n",
      "6807 2018-03-02 06:18:28     -1   [[]]  \n",
      "3778 2019-01-31 14:19:19      0     []  \n",
      "...                  ...    ...    ...  \n",
      "3290 2019-04-01 08:03:28     -1   [[]]  \n",
      "2160 2019-07-03 09:12:12      0     []  \n",
      "1037 2019-09-14 09:57:35      0     []  \n",
      "6465 2018-04-25 14:33:07      0     []  \n",
      "7231 2017-12-20 12:30:26      0     []  \n",
      "4464 2018-11-14 18:26:38      1     []  \n",
      "6982 2018-02-01 22:32:48      1     []  \n",
      "219  2019-10-27 16:24:55     -1   [[]]  \n",
      "3492 2019-03-09 17:19:10      0     []  \n",
      "1013 2019-09-16 06:03:18      0     []  \n",
      "4104 2018-12-27 14:35:46     -1   [[]]  \n",
      "4229 2018-12-14 17:18:15      1     []  \n",
      "454  2019-10-14 05:54:24      1     []  \n",
      "2705 2019-05-21 10:25:09      0     []  \n",
      "6359 2018-05-11 18:49:40     -1   [[]]  \n",
      "4645 2018-10-29 20:56:19     -1   [[]]  \n",
      "2148 2019-07-04 06:20:58      1     []  \n",
      "9237 2017-02-04 07:59:35      0     []  \n",
      "8152 2017-08-22 14:15:40      0     []  \n",
      "5350 2018-08-21 19:45:19      0     []  \n",
      "5576 2018-08-01 10:23:13      0     []  \n",
      "7925 2017-09-23 17:20:47      0     []  \n",
      "7133 2018-01-05 23:19:10      0     []  \n",
      "3679 2019-02-17 07:00:26      0     []  \n",
      "3553 2019-03-03 19:08:18      0     []  \n",
      "7622 2017-10-27 09:33:15     -1   [[]]  \n",
      "9041 2017-03-15 18:25:25      1     []  \n",
      "1079 2019-09-11 00:08:48     -1   [[]]  \n",
      "7919 2017-09-24 13:24:36      0     []  \n",
      "2891 2019-05-08 07:57:35     -1   [[]]  \n",
      "\n",
      "[1871 rows x 5 columns]\n",
      "Tweet text:\n",
      "(1871,)\n",
      "Loaded Model\n",
      "Good Words: 52084\n",
      "Bad Words: 990\n",
      "Array shape: (1801, 57, 100)\n",
      "Tensor shape: torch.Size([1801, 57, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2232    [[]]\n",
       "3067      []\n",
       "3593      []\n",
       "2776      []\n",
       "3174      []\n",
       "6567    [[]]\n",
       "7503      []\n",
       "7559      []\n",
       "882       []\n",
       "934     [[]]\n",
       "9141    [[]]\n",
       "1177      []\n",
       "5194      []\n",
       "6581      []\n",
       "1163      []\n",
       "6830    [[]]\n",
       "6129      []\n",
       "4983      []\n",
       "2052    [[]]\n",
       "8877    [[]]\n",
       "1470      []\n",
       "9313      []\n",
       "6043    [[]]\n",
       "3275      []\n",
       "3074      []\n",
       "1332      []\n",
       "3046    [[]]\n",
       "7325    [[]]\n",
       "6807    [[]]\n",
       "3778      []\n",
       "        ... \n",
       "3290    [[]]\n",
       "2160      []\n",
       "1037      []\n",
       "6465      []\n",
       "7231      []\n",
       "4464      []\n",
       "6982      []\n",
       "219     [[]]\n",
       "3492      []\n",
       "1013      []\n",
       "4104    [[]]\n",
       "4229      []\n",
       "454       []\n",
       "2705      []\n",
       "6359    [[]]\n",
       "4645    [[]]\n",
       "2148      []\n",
       "9237      []\n",
       "8152      []\n",
       "5350      []\n",
       "5576      []\n",
       "7925      []\n",
       "7133      []\n",
       "3679      []\n",
       "3553      []\n",
       "7622    [[]]\n",
       "9041      []\n",
       "1079    [[]]\n",
       "7919      []\n",
       "2891    [[]]\n",
       "Name: onehot, Length: 1871, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Generating Test Vector...\")\n",
    "test_tweet_vector, test_lengths, test_nulls = generateTweetTensor(glove, test_x)\n",
    "test_y\n",
    "#test_y_tensor = torch.LongTensor([x for x in np.array(test_y)])\n",
    "#test_y_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_y_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-24f71f6d9543>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_nulls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mi\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtest_nulls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest_y_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_y_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tweet_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_y_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "for n in range(0, len(test_nulls)):\n",
    "    i  = test_nulls[n] - n\n",
    "    test_y_tensor = torch.cat([test_y_tensor[0: i], test_y_tensor[i+1:]])\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_tweet_vector, test_y_tensor, torch.tensor(test_lengths))\n",
    "test_iter = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Generating Validate Vector...\")\n",
    "validate_tweet_vector, validate_lengths, validate_nulls = generateTweetTensor(glove, validate_x)\n",
    "validate_y_tensor = torch.LongTensor([x for x in np.array(validate_y)])\n",
    "for n in range(0, len(validate_nulls)):\n",
    "    i  = validate_nulls[n] - n\n",
    "    validate_y_tensor = torch.cat([validate_y_tensor[0: i], validate_y_tensor[i+1:]])\n",
    "val_dataset = torch.utils.data.TensorDataset(validate_tweet_vector, validate_y_tensor, torch.tensor(validate_lengths))\n",
    "#val_iter = data.BucketIterator(val_dataset, batch_size=64, repeat=False, sort_key=lambda x: len(x.text))\n",
    "val_iter = DataLoader(val_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "print(\"Generating Train Vector...\")\n",
    "train_tweet_vector, train_lengths, train_nulls = generateTweetTensor(glove, train_x)\n",
    "train_y_tensor = torch.LongTensor([x for x in np.array(train_y)])\n",
    "#actual = (torch.from_numpy(np.eye(10)[label])).float()\n",
    "\n",
    "\n",
    "for n in range(0, len(train_nulls)):\n",
    "    i  = train_nulls[n] - n\n",
    "    train_y_tensor = torch.cat([train_y_tensor[0: i], train_y_tensor[i+1:]])\n",
    "train_dataset = torch.utils.data.TensorDataset(train_tweet_vector, train_y_tensor, torch.tensor(train_lengths) )\n",
    "train_iter = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "print(\"Generated buckets.\")\n",
    "\n",
    "# Create rnn\n",
    "#model = models.RNN(100, 100)\n",
    "print(model.parameters())\n",
    "learning_rate = 0.01\n",
    "num_epochs = 25\n",
    "\n",
    "# Train models\n",
    "loss_fnc = nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "eps = []\n",
    "training_loss = []\n",
    "training_accs = []\n",
    "validation_loss = []\n",
    "validation_accs = []\n",
    "print(\"Model Created.\")\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    eps += [e]\n",
    "    print(\"Epoch: \" + str(e))\n",
    "    for j, batch in enumerate(train_iter):\n",
    "        inputs = batch[0]\n",
    "        actual = batch[1]\n",
    "        lengths_float = batch[2]\n",
    "        lengths = lengths_float.long()\n",
    "        print(\"success\")\n",
    "        optimizer.zero_grad()\n",
    "        predicted = model(inputs,lengths)\n",
    "        loss = loss_fnc(predicted, actual.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Calculating Training Accuracy...\")\n",
    "    # Calculate Training Accuracy\n",
    "    train_labels = []\n",
    "    train_preds = []\n",
    "\n",
    "\n",
    "    for k, t_batch in enumerate(train_iter):\n",
    "        t_inputs = t_batch[0]\n",
    "        t_actual = t_batch[1]\n",
    "        t_lengths_floats = t_batch[2]\n",
    "        t_lengths = t_lengths_floats.long()\n",
    "\n",
    "        train_labels += t_actual.tolist()\n",
    "        t_predicted = model(t_inputs, t_lengths)\n",
    "        train_preds += t_predicted.tolist()\n",
    "\n",
    "    t_acc = calculateAcc(train_labels, train_preds)\n",
    "    t_loss = loss_fnc(torch.FloatTensor(train_preds), torch.FloatTensor(train_labels))\n",
    "    training_accs += [t_acc]\n",
    "    training_loss += [t_loss.item()]\n",
    "    print(\"Training Accuracy: \" + str(t_acc))\n",
    "    print(\"Training Loss: \" + str(t_loss.item()))\n",
    "\n",
    "    # Calculate Validation Accuracy\n",
    "    valid_labels = []\n",
    "    valid_preds = []\n",
    "    for k, v_batch in enumerate(val_iter):\n",
    "        v_inputs = v_batch[0]\n",
    "        v_actual = v_batch[1]\n",
    "        v_lengths_floats = v_batch[2]\n",
    "        v_lengths = v_lengths_floats.long()\n",
    "        valid_labels += v_actual.tolist()\n",
    "        v_predicted = model(v_inputs, v_lengths)\n",
    "        valid_preds += v_predicted.tolist()\n",
    "\n",
    "    v_acc = calculateAcc(valid_labels, valid_preds)\n",
    "    v_loss = loss_fnc(torch.FloatTensor(valid_preds), torch.FloatTensor(valid_labels))\n",
    "    validation_accs += [v_acc]\n",
    "    validation_loss += [v_loss.item()]\n",
    "    print(\"Validation Accuracy: \" + str(v_acc))\n",
    "    print(\"Validation Loss: \" + str(v_loss.item()))\n",
    "\n",
    "# Calculate Testing Accuracy\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "for k, t_batch in enumerate(test_iter):\n",
    "    t_inputs = t_batch[0]\n",
    "    t_actual = t_batch[1]\n",
    "    t_lengths_floats = t_batch[2]\n",
    "    t_lengths = t_lengths_floats.long()\n",
    "    test_labels += t_actual.tolist()\n",
    "    t_predicted = model(t_inputs, t_lengths)\n",
    "    test_preds += t_predicted.tolist()\n",
    "\n",
    "test_loss = loss_fnc(torch.FloatTensor(test_preds), torch.FloatTensor(test_labels))\n",
    "print(\"TESTING LOSS: \" + str(test_loss.item()))\n",
    "t_acc = calculateAcc(test_labels, test_preds)\n",
    "print(\"TESTING ACCURACY: \" + str(t_acc))\n",
    "\n",
    "#   Display Accuracy vs. Epoch\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(eps, training_accs, label='Training Data')\n",
    "ax.plot(eps, validation_accs, label='Validation Data')\n",
    "ax.set(xlabel='Number of Epochs', ylabel='Accuracy', title='Accuracy vs. Epoch')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "#   Display Loss vs. Epoch\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(eps, training_loss, label='Training Data')\n",
    "ax.plot(eps, validation_loss, label='Validation Data')\n",
    "ax.set(xlabel='Number of Epochs', ylabel='Loss', title='Loss vs. Epoch')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN2, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "def forward(self, data, length, hidden):\n",
    "        length = lengths.cpu()\n",
    "        data = torch.transpose(data, 0, 1)\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(data, lengths= length, enforce_sorted = False) # unpad\n",
    "        combined = torch.cat((embeds.view(1, -1), hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
